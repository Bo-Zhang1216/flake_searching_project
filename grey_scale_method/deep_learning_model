import json
import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tqdm.keras import TqdmCallback  # progress bar callback

def load_data(json_file_path):
    """
    Loads the JSON file containing the combined and shuffled data.
    Each record is expected to be in the form:
         [I_background, I_flake, Delta_I, Ratio, label]
    Returns:
         X: A numpy array of shape (n_samples, 4) containing the features.
         y: A numpy array of shape (n_samples,) containing the labels as integers.
    """
    with open(json_file_path, "r") as f:
        data = json.load(f)
    X, y = [], []
    for record in data:
        features = record[:4]
        label = int(record[4])
        X.append(features)
        y.append(label)
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int32)

def build_complex_model(input_dim=4):
    """
    Builds a deeper neural network with increased capacity.
    The model architecture is:
        Dense(256) -> BatchNorm -> Dense(256) -> BatchNorm ->
        Dense(128) -> BatchNorm -> Dense(64) -> BatchNorm ->
        Dense(32) -> Dense(1 with sigmoid)
    """
    model = keras.Sequential([
        layers.Dense(256, activation='relu', input_shape=(input_dim,)),
        layers.BatchNormalization(),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(128, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(64, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')  # Binary classification output
    ])
    return model

def train_complex_model(json_file_path, model_filename="complex_model.h5", epochs=300, batch_size=16):
    # Load features and labels from JSON data.
    X, y = load_data(json_file_path)
    
    # Build and compile the model.
    model = build_complex_model(input_dim=X.shape[1])
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    
    model.summary()
    
    # Train the model with a progress bar.
    history = model.fit(
        X, y,
        epochs=epochs,
        batch_size=batch_size,
        verbose=0,  # Disable default verbose
        callbacks=[TqdmCallback(verbose=1)]
    )
    
    # Evaluate training accuracy.
    train_loss, train_acc = model.evaluate(X, y, verbose=0)
    print(f"\nFinal training accuracy: {train_acc:.4f}")
    
    # Save the model in the same folder as this script.
    script_dir = os.path.dirname(os.path.abspath(__file__))
    model_path = os.path.join(script_dir, model_filename)
    model.save(model_path)
    print(f"Model saved to {model_path}")
    
    return model, history

if __name__ == "__main__":
    json_file_path = "/Users/massimozhang/Desktop/coding/Ma Lab/flake_searching_project/grey_scale_method/trial_1_data/combined_shuffled_data.json"  # Adjust path if necessary
    train_complex_model(json_file_path)
